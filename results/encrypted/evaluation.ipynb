{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3cea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tenseal in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (0.3.16)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-2.3.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: psutil in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: setuptools in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Using cached triton-3.3.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Using cached numpy-2.3.0-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached torch-2.7.1-cp313-cp313-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, hf-xet, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [accelerate]9\u001b[0m [accelerate]s]er-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.3 huggingface-hub-0.32.4 mpmath-1.3.0 networkx-3.5 numpy-2.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 regex-2024.11.6 safetensors-0.5.3 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.1 transformers-4.52.4 triton-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tenseal\n",
    "!pip install transformers accelerate torch\n",
    "!pip install -q --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08b07f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /net/afscra/people/plgadamwo2002/kr/lib/python3.13/site-packages (from scikit-learn) (2.3.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8148caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import hashlib\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tenseal as ts\n",
    "import hashlib\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "# --- Configuration ---\n",
    "BUCKETS = 1024\n",
    "NGRAM_SIZE = 4\n",
    "SCALE = 2 ** 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_ngram(ngram, buckets=1024):\n",
    "    return int(hashlib.sha256(ngram.encode()).hexdigest(), 16) % buckets\n",
    "\n",
    "def vectorize(text, n=4, buckets=1024):\n",
    "    vec = [0] * buckets\n",
    "    text = text.lower()\n",
    "    for i in range(len(text) - n + 1):\n",
    "        idx = hash_ngram(text[i:i+n], buckets)\n",
    "        vec[idx] += 1\n",
    "    return vec\n",
    "\n",
    "# Connect to OpenRouter using their API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# 1. Helper: LLM phrase generation from regex\n",
    "def generate_phrases_from_regex(regex: str, max_phrases=10) -> List[str]:\n",
    "    prompt = f\"\"\"You are a cybersecurity expert. Given the following regex from a phishing YARA rule:\n",
    "\n",
    "{regex}\n",
    "\n",
    "List {max_phrases} natural phrases or sentences that could appear in a phishing email and match the intent of this regex.\n",
    "Just list each phrase on a new line without explanations.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    raw_text = response.choices[0].message.content\n",
    "\n",
    "    phrases = [\n",
    "        re.sub(r\"^\\s*[\\d]+[\\.\\)\\-]*\\s*\", \"\", line).strip(\" -•\\n\").lstrip(\"\\\"\")\n",
    "        for line in raw_text.split(\"\\n\") if line.strip()\n",
    "    ]\n",
    "    return phrases\n",
    "\n",
    "# 2. Helper: extract n-grams (3-5 words) from phrases\n",
    "def extract_ngrams(text: str, min_n=3, max_n=5) -> List[str]:\n",
    "    words = text.lower().split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+n])\n",
    "        for n in range(min_n, max_n + 1)\n",
    "        for i in range(len(words) - n + 1)\n",
    "    ]\n",
    "\n",
    "# 3. Convert YARA regexes to vectorizer and feature space\n",
    "def vectorize_yara_phrases(rules: List[Dict[str, object]]) -> Tuple[TfidfVectorizer, List[str]]:\n",
    "    all_phrases = []\n",
    "    for rule in rules:\n",
    "        pattern = rule[\"pattern\"]\n",
    "        phrases = generate_phrases_from_regex(pattern)\n",
    "        all_phrases.extend(phrases)\n",
    "\n",
    "    # Convert to 3–5 word n-grams\n",
    "    ngrams = set()\n",
    "    for phrase in all_phrases:\n",
    "        ngrams.update(extract_ngrams(phrase, 3, 5))\n",
    "\n",
    "    sorted_vocab = sorted(ngrams)\n",
    "    vectorized_vocab = [vectorize(phrase) for phrase in sorted_vocab]\n",
    "\n",
    "    return vectorized_vocab, sorted_vocab\n",
    "\n",
    "\n",
    "\n",
    "def generate_regexs_from_rule_text(rule_text: str, max_regex=3) -> List[Dict[str, str]]:\n",
    " \n",
    "    prompt = f\"\"\"You are a cybersecurity expert. Here is a YARA rule:\n",
    "    {rule_text}\n",
    "\n",
    "    Your task is to extract up to {max_regex} regular expressions that are explicitly written or clearly implied by the rule.\n",
    "\n",
    "    For each regex, estimate the \"weight\" field (as a float) to reflect how strongly the pattern indicates phishing (the higher the weight, the more suspicious/phishy the pattern is). Assign higher weights to patterns that are more typical for phishing, and lower weights to more generic or less suspicious patterns.\n",
    "\n",
    "    Return the result as a valid Python list of dictionaries, using the following exact format:\n",
    "    [\n",
    "        {{ \"pattern\": r\"/regex1/i\", \"weight\": 1.0 }},\n",
    "        {{ \"pattern\": r\"/regex2/i\", \"weight\": 0.7 }}\n",
    "    ]\n",
    "\n",
    "    Do not include any explanations, markdown, or comments!\n",
    "\n",
    "    - Each pattern must be a Python raw string (starting with r\") and contain a YARA-style regex: between slashes (e.g., /something/i).\n",
    "    - Do NOT use extra quotes inside the pattern (e.g., no \"r\\\"...\\\"\" or escapes).\n",
    "    - Do NOT include any markdown, explanations, comments, or extra text — just the list.\n",
    "    - If the rule contains no regex, infer up to 3 regex-style patterns that match the intent of the rule (e.g., suspicious links, phishing phrases, etc.).\n",
    "    - The pattern must be a valid Python raw string.\n",
    "    - Avoid unescaped quotes. Prefer single quotes `'` around strings.\n",
    "    - Do not include YARA flags like `nocase` as raw identifiers — use `/.../i` inside the string instead.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    raw_output = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        return eval(raw_output.strip())  # UWAGA: tylko jeśli masz pełną kontrolę nad odpowiedzią\n",
    "    except Exception as e:\n",
    "        print(\"Błąd parsowania odpowiedzi z LLM:\", e)\n",
    "        print(\"Odpowiedź:\\n\", raw_output)\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_regexs_from_folder(folder_path: str, max_regex=3):\n",
    "    \"\"\"\n",
    "    Zwraca listę krotek (nazwa_pliku, lista_reguł)\n",
    "    \"\"\"\n",
    "    all_rule_sets = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".yar\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            rule_text = f.read()\n",
    "\n",
    "        regex_list = generate_regexs_from_rule_text(rule_text, max_regex=max_regex)\n",
    "        if regex_list:\n",
    "            all_rule_sets.append((filename, regex_list))\n",
    "        else:\n",
    "            print(f\"Brak regexów w pliku {filename}\")\n",
    "\n",
    "    return all_rule_sets\n",
    "\n",
    "# --- Text Hashing Vectorizer ---\n",
    "def hash_ngram(ngram, buckets=BUCKETS):\n",
    "    return int(hashlib.sha256(ngram.encode()).hexdigest(), 16) % buckets\n",
    "\n",
    "def vectorize(text, n=NGRAM_SIZE, buckets=BUCKETS):\n",
    "    vec = [0] * buckets\n",
    "    text = text.lower()\n",
    "    for i in range(len(text) - n + 1):\n",
    "        idx = hash_ngram(text[i:i+n], buckets)\n",
    "        vec[idx] += 1\n",
    "    return vec\n",
    "\n",
    "# --- CKKS Context ---\n",
    "def create_ckks_context():\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60]  # Must match scale size\n",
    "    )\n",
    "    context.global_scale = SCALE\n",
    "    context.generate_galois_keys()\n",
    "    return context\n",
    "\n",
    "# --- Encryption Helper ---\n",
    "def encrypt_vector(vector, context):\n",
    "    return ts.ckks_vector(context, vector)\n",
    "\n",
    "def rule_matches(pattern: str, text: str) -> bool:\n",
    "    return re.search(pattern.strip(\"/i\"), text, re.IGNORECASE) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce17830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_all_emails(input_dir: str, output_path: str) -> None:\n",
    "    input_dir = Path(input_dir)\n",
    "    vectors: list[np.ndarray] = []\n",
    "    for dirpath, dirnames, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            with open(file_path, encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "                text = file.read()\n",
    "                vec = vectorize(text)\n",
    "                vectors.append(np.array(vec))\n",
    "\n",
    "    np.savez_compressed(output_path, np.array(vectors))\n",
    "    \n",
    "    return np.array(vectors)\n",
    "\n",
    "def iter_emails(path: str):\n",
    "    array = np.load(path)['arr_0']\n",
    "    for row in array:\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_threshold(regular_path, vectorized_vocab, vocab_boost=2.0, percentile=75):\n",
    "    context = create_ckks_context()\n",
    "    max_len = max(max(len(vec) for vec in vectorized_vocab), BUCKETS)\n",
    "\n",
    "    padded_vocab = [vec + [0] * (max_len - len(vec)) for vec in vectorized_vocab]\n",
    "    encrypted_vocab = [encrypt_vector(vec, context) for vec in padded_vocab]\n",
    "\n",
    "    def evaluate(vec):\n",
    "        vec = list(vec) + [0] * (max_len - len(vec))\n",
    "        enc_email = encrypt_vector(vec, context)\n",
    "        similarity_scores = [enc_email.dot(enc_phrase).decrypt()[0] for enc_phrase in encrypted_vocab]\n",
    "        return max(similarity_scores)\n",
    "\n",
    "    similarities = []\n",
    "    for vec in iter_emails(regular_path):\n",
    "        similarities.append(evaluate(vec))\n",
    "\n",
    "    import numpy as np\n",
    "    base_threshold = np.percentile(similarities, percentile)\n",
    "\n",
    "    return base_threshold + vocab_boost\n",
    "\n",
    "def scan_vectorized_emails(\n",
    "    phishing_path: str,\n",
    "    regular_path: str,\n",
    "    output_path: str,\n",
    "    rules: List[Dict[str, object]],\n",
    "    vectorized_vocab: List[List[int]],\n",
    "    threshold: float = 10.0,\n",
    "    vocab_boost: float = 2.0,\n",
    "    sample_fraction: float = 1.0,\n",
    "    max_files: int = None\n",
    "):\n",
    "    import random\n",
    "\n",
    "    context = create_ckks_context()\n",
    "    max_len = max(max(len(vec) for vec in vectorized_vocab), BUCKETS)\n",
    "\n",
    "    padded_vocab = [vec + [0] * (max_len - len(vec)) for vec in vectorized_vocab]\n",
    "    encrypted_vocab = [encrypt_vector(vec, context) for vec in padded_vocab]\n",
    "\n",
    "    def evaluate_email(vec):\n",
    "        vec = list(vec) + [0] * (max_len - len(vec))\n",
    "        enc_email = encrypt_vector(vec, context)\n",
    "        similarity_scores = [enc_email.dot(enc_phrase).decrypt()[0] for enc_phrase in encrypted_vocab]\n",
    "        return max(similarity_scores)\n",
    "\n",
    "    if threshold == \"auto\":\n",
    "        threshold = calibrate_threshold(\n",
    "            regular_path, vectorized_vocab,\n",
    "            vocab_boost=vocab_boost\n",
    "        )\n",
    "        print(f\"Dynamicznie ustawiony próg: {threshold:.2f}\")\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for label, path in [(\"phishing_mails\", phishing_path), (\"regular_mails\", regular_path)]:\n",
    "            all_vecs = list(iter_emails(path))\n",
    "            random.shuffle(all_vecs)\n",
    "            sample_size = min(int(len(all_vecs) * sample_fraction), max_files or len(all_vecs))\n",
    "\n",
    "            for idx, vec in enumerate(all_vecs[:sample_size]):\n",
    "                similarity = evaluate_email(vec)\n",
    "                yara_score = 0.0\n",
    "                text = \"\"  # optionally left empty\n",
    "                for rule in rules:\n",
    "                    if rule_matches(rule[\"pattern\"], text):\n",
    "                        yara_score += rule[\"weight\"]\n",
    "\n",
    "                final_score = similarity + yara_score\n",
    "                final_threshold = threshold\n",
    "                result = \"MATCH\" if final_score >= final_threshold else \"NO MATCH\"\n",
    "\n",
    "                f.write(f\"{result}: {label}/{idx} -> similarity={similarity:.2f}, yara_score={yara_score:.2f}, total={final_score:.2f}\\n\")\n",
    "\n",
    "\n",
    "def analyze_vectorized_results(result_file_path: str):\n",
    "    phishing_matches = phishing_total = 0\n",
    "    regular_matches = regular_total = 0\n",
    "    other_matches = other_total = 0\n",
    "\n",
    "    with open(result_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            is_match = line.startswith(\"MATCH\")\n",
    "            if \"phishing_mails\" in line:\n",
    "                phishing_total += 1\n",
    "                if is_match:\n",
    "                    phishing_matches += 1\n",
    "            elif \"regular_mails\" in line:\n",
    "                regular_total += 1\n",
    "                if is_match:\n",
    "                    regular_matches += 1\n",
    "            else:\n",
    "                other_total += 1\n",
    "                if is_match:\n",
    "                    other_matches += 1\n",
    "\n",
    "    total_matches = phishing_matches + regular_matches + other_matches\n",
    "    total_scanned = phishing_total + regular_total + other_total\n",
    "\n",
    "    print(\"Analiza wyników (vectorized approach):\")\n",
    "    print(f\"Phishing mails: {phishing_matches} / {phishing_total} dopasowań\")\n",
    "    print(f\"Regular mails:  {regular_matches} / {regular_total} dopasowań\")\n",
    "    print(f\"Inne pliki:      {other_matches} / {other_total} dopasowań\")\n",
    "    print(f\"SUMA:            {total_matches} / {total_scanned} plików dopasowanych\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb8867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zestaw reguł z pliku: sekurak_yara_example.yar ---\n",
      "Regexy w zestawie:\n",
      "  sekret\n",
      "  password\n",
      "  .*sekret.*\n",
      "\n",
      "--- Zestaw reguł z pliku: complex_html_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  <!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01\n",
      "  <a[^>]+>(\\s*<\\/a>|\\s*<span.*<\\/span><\\/a>)\n",
      "  Hello,\\s*nocase\n",
      "\n",
      "--- Zestaw reguł z pliku: encoded_reply_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  /Reply-To:\\s+=\\?UTF-8\\?B\\?.{20,}\\?=/i\n",
      "\n",
      "--- Zestaw reguł z pliku: suspicious_tld_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  $tld_ml|$tld_ga|$tld_ru|$tld_su|$tld_skin|$tld_cn|$tld_top|$tld_sbs|$tld_bond|$tld_xyz\n",
      "  \\.(ml|ga|ru|su|skin|cn|top|sbs|bond|xyz)$\n",
      "  $tld*\n",
      "\n",
      "--- Zestaw reguł z pliku: suspicious_links_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  bit\\.ly/\n",
      "  tinyurl\\.com/\n",
      "  https?://[^ ]*@(.*)\n",
      "\n",
      "--- Zestaw reguł z pliku: phrases_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  Renew your subscription|Update your payment details|Your shipment is on the way|Password Expiration Notification|New file shared in Teams|Urgent|Verification required|Invoice|Need urgent help|Suspicious Outlook activity|Important! Your password is about to expire|Action required|Click below\n",
      "  phishing\\s*email\\b\n",
      "  [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\n",
      "\n",
      "--- Zestaw reguł z pliku: domains_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  chainsmokers\\-feeling\\.org\n",
      "  xfund02\\.ml\n",
      "  circularhub\\.ch\n"
     ]
    }
   ],
   "source": [
    "# -- YARA rules with weights for every file in the \"rules\" folder ---\n",
    "rules_sets = generate_regexs_from_folder(\"rules/\", max_regex=3)\n",
    "\n",
    "all_vectorized_vocabs = []  # lista: [vectorized_vocab dla każdego pliku]\n",
    "all_sorted_vocabs = []      # lista: [sorted_vocab dla każdego pliku]\n",
    "all_filenames = []          # lista nazw plików \n",
    "all_rules = []              # lista reguł \n",
    "\n",
    "for filename, rules in rules_sets:\n",
    "    print(f\"\\n--- Zestaw reguł z pliku: {filename} ---\")\n",
    "    print(\"Regexy w zestawie:\")\n",
    "    for rule in rules:\n",
    "        print(f\"  {rule['pattern']}\")\n",
    "    vectorized_vocab, sorted_vocab = vectorize_yara_phrases(rules)\n",
    "    all_vectorized_vocabs.append(vectorized_vocab)\n",
    "    all_sorted_vocabs.append(sorted_vocab)\n",
    "    all_filenames.append(filename)\n",
    "    all_rules.extend(rules)\n",
    "    # print(\"Lista fraz:\", sorted_vocab)\n",
    "\n",
    "\n",
    "# -- YARA rules with weights for all files in the \"rules\" folder ---\n",
    "global_vectorized_vocab, global_sorted_vocab = vectorize_yara_phrases(all_rules)\n",
    "# print(\"\\n=== GLOBALNA LISTA SŁOWNIKÓW FRAZ (ze wszystkich plików) ===\")\n",
    "# print(global_sorted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---All YARA files processing in loop--\n",
    "def process_rule_set(filename, rules, vectorized_vocab):\n",
    "    print(f\"\\n--- Przetwarzanie reguł z pliku: {filename} ---\")\n",
    "    output_path = f\"results/{filename}_results.txt\"\n",
    "\n",
    "    scan_vectorized_emails(\n",
    "        phishing_path=\"./samples/phishing_mails_vectorized.npz\",\n",
    "        regular_path=\"./samples/regular_mails_vectorized.npz\",\n",
    "        output_path=output_path,\n",
    "        rules=rules,\n",
    "        vectorized_vocab=vectorized_vocab,\n",
    "        threshold=\"auto\",\n",
    "        vocab_boost=2.0\n",
    "    )\n",
    "\n",
    "    analyze_vectorized_results(output_path)\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    (filename, rules, vocab)\n",
    "    for (filename, rules), vocab in zip(rules_sets, all_vectorized_vocabs)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd17f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Przetwarzanie reguł z pliku: sekurak_yara_example.yar ---\n",
      "\n",
      "\n",
      "--- Przetwarzanie reguł z pliku: encoded_reply_rule.yar ---\n",
      "\n",
      "--- Przetwarzanie reguł z pliku: complex_html_rule.yar ---\n",
      "--- Przetwarzanie reguł z pliku: suspicious_tld_rule.yar ---\n",
      "\n",
      "--- Przetwarzanie reguł z pliku: suspicious_links_rule.yar ---\n",
      "\n",
      "--- Przetwarzanie reguł z pliku: phrases_rule.yar ---\n",
      "\n",
      "--- Przetwarzanie reguł z pliku: domains_rule.yar ---\n",
      "Dynamicznie ustawiony próg: 692.00\n",
      "Dynamicznie ustawiony próg: 620.00\n",
      "Dynamicznie ustawiony próg: 698.50\n",
      "Dynamicznie ustawiony próg: 466.00\n",
      "Dynamicznie ustawiony próg: 724.50\n",
      "Dynamicznie ustawiony próg: 604.00\n",
      "Analiza wyników (vectorized approach):\n",
      "Phishing mails: 611 / 822 dopasowań\n",
      "Regular mails:  221 / 887 dopasowań\n",
      "Inne pliki:      0 / 0 dopasowań\n",
      "SUMA:            832 / 1709 plików dopasowanych\n",
      "\n",
      "Analiza wyników (vectorized approach):\n",
      "Phishing mails: 549 / 822 dopasowań\n",
      "Regular mails:  222 / 887 dopasowań\n",
      "Inne pliki:      0 / 0 dopasowań\n",
      "SUMA:            771 / 1709 plików dopasowanych\n",
      "\n",
      "Analiza wyników (vectorized approach):\n",
      "Phishing mails: 631 / 822 dopasowań\n",
      "Regular mails:  221 / 887 dopasowań\n",
      "Inne pliki:      0 / 0 dopasowań\n",
      "SUMA:            852 / 1709 plików dopasowanych\n",
      "\n",
      "Analiza wyników (vectorized approach):\n",
      "Phishing mails: 623 / 822 dopasowań\n",
      "Regular mails:  220 / 887 dopasowań\n",
      "Inne pliki:      0 / 0 dopasowań\n",
      "SUMA:            843 / 1709 plików dopasowanych\n",
      "\n",
      "Analiza wyników (vectorized approach):\n",
      "Phishing mails: 528 / 822 dopasowań\n",
      "Regular mails:  221 / 887 dopasowań\n",
      "Inne pliki:      0 / 0 dopasowań\n",
      "SUMA:            749 / 1709 plików dopasowanych\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        pool.starmap(process_rule_set, tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
