{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8148caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import hashlib\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tenseal as ts\n",
    "import hashlib\n",
    "\n",
    "# --- Configuration ---\n",
    "BUCKETS = 1024\n",
    "NGRAM_SIZE = 4\n",
    "SCALE = 2 ** 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_ngram(ngram, buckets=1024):\n",
    "    return int(hashlib.sha256(ngram.encode()).hexdigest(), 16) % buckets\n",
    "\n",
    "def vectorize(text, n=4, buckets=1024):\n",
    "    vec = [0] * buckets\n",
    "    text = text.lower()\n",
    "    for i in range(len(text) - n + 1):\n",
    "        idx = hash_ngram(text[i:i+n], buckets)\n",
    "        vec[idx] += 1\n",
    "    return vec\n",
    "\n",
    "# Connect to OpenRouter using their API key and base URL\n",
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# 1. Helper: LLM phrase generation from regex\n",
    "def generate_phrases_from_regex(regex: str, max_phrases=10) -> List[str]:\n",
    "    prompt = f\"\"\"You are a cybersecurity expert. Given the following regex from a phishing YARA rule:\n",
    "\n",
    "{regex}\n",
    "\n",
    "List {max_phrases} natural phrases or sentences that could appear in a phishing email and match the intent of this regex.\n",
    "Just list each phrase on a new line without explanations.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    raw_text = response.choices[0].message.content\n",
    "\n",
    "    phrases = [\n",
    "        re.sub(r\"^\\s*[\\d]+[\\.\\)\\-]*\\s*\", \"\", line).strip(\" -•\\n\").lstrip(\"\\\"\")\n",
    "        for line in raw_text.split(\"\\n\") if line.strip()\n",
    "    ]\n",
    "    return phrases\n",
    "\n",
    "# 2. Helper: extract n-grams (3-5 words) from phrases\n",
    "def extract_ngrams(text: str, min_n=3, max_n=5) -> List[str]:\n",
    "    words = text.lower().split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+n])\n",
    "        for n in range(min_n, max_n + 1)\n",
    "        for i in range(len(words) - n + 1)\n",
    "    ]\n",
    "\n",
    "# 3. Convert YARA regexes to vectorizer and feature space\n",
    "def vectorize_yara_phrases(rules: List[Dict[str, object]]) -> Tuple[TfidfVectorizer, List[str]]:\n",
    "    all_phrases = []\n",
    "    for rule in rules:\n",
    "        pattern = rule[\"pattern\"]\n",
    "        phrases = generate_phrases_from_regex(pattern)\n",
    "        all_phrases.extend(phrases)\n",
    "\n",
    "    # Convert to 3–5 word n-grams\n",
    "    ngrams = set()\n",
    "    for phrase in all_phrases:\n",
    "        ngrams.update(extract_ngrams(phrase, 3, 5))\n",
    "\n",
    "    sorted_vocab = sorted(ngrams)\n",
    "    vectorized_vocab = [vectorize(phrase) for phrase in sorted_vocab]\n",
    "\n",
    "    return vectorized_vocab, sorted_vocab\n",
    "\n",
    "\n",
    "\n",
    "def generate_regexs_from_rule_text(rule_text: str, max_regex=3) -> List[Dict[str, str]]:\n",
    " \n",
    "    prompt = f\"\"\"You are a cybersecurity expert. Here is a YARA rule:\n",
    "    {rule_text}\n",
    "\n",
    "    Your task is to extract up to {max_regex} regular expressions that are explicitly written or clearly implied by the rule.\n",
    "\n",
    "    For each regex, estimate the \"weight\" field (as a float) to reflect how strongly the pattern indicates phishing (the higher the weight, the more suspicious/phishy the pattern is). Assign higher weights to patterns that are more typical for phishing, and lower weights to more generic or less suspicious patterns.\n",
    "\n",
    "    Return the result as a valid Python list of dictionaries, using the following exact format:\n",
    "    [\n",
    "        {{ \"pattern\": r\"/regex1/i\", \"weight\": 1.0 }},\n",
    "        {{ \"pattern\": r\"/regex2/i\", \"weight\": 0.7 }}\n",
    "    ]\n",
    "\n",
    "    Do not include any explanations, markdown, or comments!\n",
    "\n",
    "    - Each pattern must be a Python raw string (starting with r\") and contain a YARA-style regex: between slashes (e.g., /something/i).\n",
    "    - Do NOT use extra quotes inside the pattern (e.g., no \"r\\\"...\\\"\" or escapes).\n",
    "    - Do NOT include any markdown, explanations, comments, or extra text — just the list.\n",
    "    - If the rule contains no regex, infer up to 3 regex-style patterns that match the intent of the rule (e.g., suspicious links, phishing phrases, etc.).\n",
    "    - The pattern must be a valid Python raw string.\n",
    "    - Avoid unescaped quotes. Prefer single quotes `'` around strings.\n",
    "    - Do not include YARA flags like `nocase` as raw identifiers — use `/.../i` inside the string instead.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-7b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    raw_output = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        return eval(raw_output.strip())  # UWAGA: tylko jeśli masz pełną kontrolę nad odpowiedzią\n",
    "    except Exception as e:\n",
    "        print(\"Błąd parsowania odpowiedzi z LLM:\", e)\n",
    "        print(\"Odpowiedź:\\n\", raw_output)\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_regexs_from_folder(folder_path: str, max_regex=3):\n",
    "    \"\"\"\n",
    "    Zwraca listę krotek (nazwa_pliku, lista_reguł)\n",
    "    \"\"\"\n",
    "    all_rule_sets = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".yar\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            rule_text = f.read()\n",
    "\n",
    "        regex_list = generate_regexs_from_rule_text(rule_text, max_regex=max_regex)\n",
    "        if regex_list:\n",
    "            all_rule_sets.append((filename, regex_list))\n",
    "        else:\n",
    "            print(f\"Brak regexów w pliku {filename}\")\n",
    "\n",
    "    return all_rule_sets\n",
    "\n",
    "# --- Text Hashing Vectorizer ---\n",
    "def hash_ngram(ngram, buckets=BUCKETS):\n",
    "    return int(hashlib.sha256(ngram.encode()).hexdigest(), 16) % buckets\n",
    "\n",
    "def vectorize(text, n=NGRAM_SIZE, buckets=BUCKETS):\n",
    "    vec = [0] * buckets\n",
    "    text = text.lower()\n",
    "    for i in range(len(text) - n + 1):\n",
    "        idx = hash_ngram(text[i:i+n], buckets)\n",
    "        vec[idx] += 1\n",
    "    return vec\n",
    "\n",
    "# --- CKKS Context ---\n",
    "def create_ckks_context():\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60]  # Must match scale size\n",
    "    )\n",
    "    context.global_scale = SCALE\n",
    "    context.generate_galois_keys()\n",
    "    return context\n",
    "\n",
    "# --- Encryption Helper ---\n",
    "def encrypt_vector(vector, context):\n",
    "    return ts.ckks_vector(context, vector)\n",
    "\n",
    "def rule_matches(pattern: str, text: str) -> bool:\n",
    "    return re.search(pattern.strip(\"/i\"), text, re.IGNORECASE) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce17830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_all_emails(input_dir: str, output_path: str) -> None:\n",
    "    input_dir = Path(input_dir)\n",
    "    vectors: list[np.ndarray] = []\n",
    "    for dirpath, dirnames, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            with open(file_path, encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "                text = file.read()\n",
    "                vec = vectorize(text)\n",
    "                vectors.append(np.array(vec))\n",
    "\n",
    "    np.savez_compressed(output_path, np.array(vectors))\n",
    "    \n",
    "    return np.array(vectors)\n",
    "\n",
    "def iter_emails(path: str):\n",
    "    array = np.load(path)['arr_0']\n",
    "    for row in array:\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_threshold(regular_path, vectorized_vocab, vocab_boost=2.0, percentile=75):\n",
    "    context = create_ckks_context()\n",
    "    max_len = max(max(len(vec) for vec in vectorized_vocab), BUCKETS)\n",
    "\n",
    "    padded_vocab = [vec + [0] * (max_len - len(vec)) for vec in vectorized_vocab]\n",
    "    encrypted_vocab = [encrypt_vector(vec, context) for vec in padded_vocab]\n",
    "\n",
    "    def evaluate(vec):\n",
    "        vec = list(vec) + [0] * (max_len - len(vec))\n",
    "        enc_email = encrypt_vector(vec, context)\n",
    "        similarity_scores = [enc_email.dot(enc_phrase).decrypt()[0] for enc_phrase in encrypted_vocab]\n",
    "        return max(similarity_scores)\n",
    "\n",
    "    similarities = []\n",
    "    for vec in iter_emails(regular_path):\n",
    "        similarities.append(evaluate(vec))\n",
    "\n",
    "    import numpy as np\n",
    "    base_threshold = np.percentile(similarities, percentile)\n",
    "\n",
    "    return base_threshold + vocab_boost\n",
    "\n",
    "def scan_vectorized_emails(\n",
    "    phishing_path: str,\n",
    "    regular_path: str,\n",
    "    output_path: str,\n",
    "    rules: List[Dict[str, object]],\n",
    "    vectorized_vocab: List[List[int]],\n",
    "    threshold: float = 10.0,\n",
    "    vocab_boost: float = 2.0,\n",
    "    sample_fraction: float = 1.0,\n",
    "    max_files: int = None\n",
    "):\n",
    "    import random\n",
    "\n",
    "    context = create_ckks_context()\n",
    "    max_len = max(max(len(vec) for vec in vectorized_vocab), BUCKETS)\n",
    "\n",
    "    padded_vocab = [vec + [0] * (max_len - len(vec)) for vec in vectorized_vocab]\n",
    "    encrypted_vocab = [encrypt_vector(vec, context) for vec in padded_vocab]\n",
    "\n",
    "    def evaluate_email(vec):\n",
    "        vec = list(vec) + [0] * (max_len - len(vec))\n",
    "        enc_email = encrypt_vector(vec, context)\n",
    "        similarity_scores = [enc_email.dot(enc_phrase).decrypt()[0] for enc_phrase in encrypted_vocab]\n",
    "        return max(similarity_scores)\n",
    "\n",
    "    if threshold == \"auto\":\n",
    "        threshold = calibrate_threshold(\n",
    "            regular_path, vectorized_vocab,\n",
    "            vocab_boost=vocab_boost\n",
    "        )\n",
    "        print(f\"Dynamicznie ustawiony próg: {threshold:.2f}\")\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for label, path in [(\"phishing_mails\", phishing_path), (\"regular_mails\", regular_path)]:\n",
    "            all_vecs = list(iter_emails(path))\n",
    "            random.shuffle(all_vecs)\n",
    "            sample_size = min(int(len(all_vecs) * sample_fraction), max_files or len(all_vecs))\n",
    "\n",
    "            for idx, vec in enumerate(all_vecs[:sample_size]):\n",
    "                similarity = evaluate_email(vec)\n",
    "                yara_score = 0.0\n",
    "                text = \"\"  # optionally left empty\n",
    "                for rule in rules:\n",
    "                    if rule_matches(rule[\"pattern\"], text):\n",
    "                        yara_score += rule[\"weight\"]\n",
    "\n",
    "                final_score = similarity + yara_score\n",
    "                final_threshold = threshold\n",
    "                result = \"MATCH\" if final_score >= final_threshold else \"NO MATCH\"\n",
    "\n",
    "                f.write(f\"{result}: {label}/{idx} -> similarity={similarity:.2f}, yara_score={yara_score:.2f}, total={final_score:.2f}\\n\")\n",
    "\n",
    "\n",
    "def analyze_vectorized_results(result_file_path: str):\n",
    "    phishing_matches = phishing_total = 0\n",
    "    regular_matches = regular_total = 0\n",
    "    other_matches = other_total = 0\n",
    "\n",
    "    with open(result_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            is_match = line.startswith(\"MATCH\")\n",
    "            if \"phishing_mails\" in line:\n",
    "                phishing_total += 1\n",
    "                if is_match:\n",
    "                    phishing_matches += 1\n",
    "            elif \"regular_mails\" in line:\n",
    "                regular_total += 1\n",
    "                if is_match:\n",
    "                    regular_matches += 1\n",
    "            else:\n",
    "                other_total += 1\n",
    "                if is_match:\n",
    "                    other_matches += 1\n",
    "\n",
    "    total_matches = phishing_matches + regular_matches + other_matches\n",
    "    total_scanned = phishing_total + regular_total + other_total\n",
    "\n",
    "    print(\"Analiza wyników (vectorized approach):\")\n",
    "    print(f\"Phishing mails: {phishing_matches} / {phishing_total} dopasowań\")\n",
    "    print(f\"Regular mails:  {regular_matches} / {regular_total} dopasowań\")\n",
    "    print(f\"Inne pliki:      {other_matches} / {other_total} dopasowań\")\n",
    "    print(f\"SUMA:            {total_matches} / {total_scanned} plików dopasowanych\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb8867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zestaw reguł z pliku: complex_html_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  <!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01\n",
      "  <a[^>]+>\\s*(?!\\/a>).*?(?=\\s*<\\/a>|\\s*<span.*<\\/span><\\/a>)\n",
      "  Hello,\\s*\n",
      "\n",
      "--- Zestaw reguł z pliku: domains_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  chainsmokers-feeling\\.org\n",
      "  xfund02\\.ml\n",
      "  (circularhub|panonika)\\.([a-z]{2,3})\n",
      "\n",
      "--- Zestaw reguł z pliku: encoded_reply_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  /Reply-To:\\s+=\\?UTF-8\\?B\\?.{20,}\\?=/i\n",
      "  /Reply-To:\\s+=\\?.*\\?=/i\n",
      "  /Reply-To: \\w+@[a-z]+.[a-z]{2,}/i\n",
      "\n",
      "--- Zestaw reguł z pliku: phrases_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  Renew your subscription|Update your payment details|Your shipment is on the way|Password Expiration Notification|New file shared in Teams|Urgent|Verification required|Invoice|Need urgent help|Suspicious Outlook activity|Important! Your password is about to expire|Action required|Click below\n",
      "  Password|payment|shipment|invoice\n",
      "  click\n",
      "\n",
      "--- Zestaw reguł z pliku: sekurak_yara_example.yar ---\n",
      "Regexy w zestawie:\n",
      "  sekret\n",
      "  password\n",
      "\n",
      "--- Zestaw reguł z pliku: suspicious_links_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  bit\\.ly\\/\n",
      "  tinyurl\\.com\\/\n",
      "  https?://[^ ]*@(.*)\n",
      "\n",
      "--- Zestaw reguł z pliku: suspicious_tld_rule.yar ---\n",
      "Regexy w zestawie:\n",
      "  (?i)\\.(ml|ga|ru|su|skin|cn|top|sbs|bond|xyz)$\n",
      "  (?i)\\.(ml|ga|ru|su|skin|cn|top|sbs|bond)$\n",
      "  (?i)\\.(xyz)$\n"
     ]
    }
   ],
   "source": [
    "# -- YARA rules with weights for every file in the \"rules\" folder ---\n",
    "rules_sets = generate_regexs_from_folder(\"rules/\", max_regex=3)\n",
    "\n",
    "all_vectorized_vocabs = []  # lista: [vectorized_vocab dla każdego pliku]\n",
    "all_sorted_vocabs = []      # lista: [sorted_vocab dla każdego pliku]\n",
    "all_filenames = []          # lista nazw plików \n",
    "all_rules = []              # lista reguł \n",
    "\n",
    "for filename, rules in rules_sets:\n",
    "    print(f\"\\n--- Zestaw reguł z pliku: {filename} ---\")\n",
    "    print(\"Regexy w zestawie:\")\n",
    "    for rule in rules:\n",
    "        print(f\"  {rule['pattern']}\")\n",
    "    vectorized_vocab, sorted_vocab = vectorize_yara_phrases(rules)\n",
    "    all_vectorized_vocabs.append(vectorized_vocab)\n",
    "    all_sorted_vocabs.append(sorted_vocab)\n",
    "    all_filenames.append(filename)\n",
    "    all_rules.extend(rules)\n",
    "    # print(\"Lista fraz:\", sorted_vocab)\n",
    "\n",
    "\n",
    "# -- YARA rules with weights for all files in the \"rules\" folder ---\n",
    "global_vectorized_vocab, global_sorted_vocab = vectorize_yara_phrases(all_rules)\n",
    "# print(\"\\n=== GLOBALNA LISTA SŁOWNIKÓW FRAZ (ze wszystkich plików) ===\")\n",
    "# print(global_sorted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Przetwarzanie reguł z pliku: complex_html_rule.yar ---\n"
     ]
    }
   ],
   "source": [
    "# ---All YARA files processing in loop--\n",
    "for (filename, rules), vectorized_vocab in zip(rules_sets, all_vectorized_vocabs):\n",
    "    print(f\"\\n--- Przetwarzanie reguł z pliku: {filename} ---\")\n",
    "    output_path = f\"results/{filename}_results.txt\"\n",
    "    \n",
    "    # print(len(all_vectorized_vocabs))\n",
    "    scan_vectorized_emails(\n",
    "        phishing_path=\"./samples/phishing_mails_vectorized.npz\",\n",
    "        regular_path=\"./samples/regular_mails_vectorized.npz\",\n",
    "        output_path=output_path,\n",
    "        rules=rules,\n",
    "        vectorized_vocab=vectorized_vocab,\n",
    "        threshold=\"auto\",\n",
    "        vocab_boost=2.0\n",
    "    )\n",
    "    \n",
    "    analyze_vectorized_results(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
