{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb56JxktWFte",
        "outputId": "106d7736-709e-4f0d-c92f-141121bebd31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.16-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
            "Downloading tenseal-0.3.16-cp312-cp312-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/2.2 MB 435.7 kB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.1/2.2 MB 751.6 kB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 0.2/2.2 MB 1.3 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 0.3/2.2 MB 1.3 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 0.4/2.2 MB 1.6 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 0.5/2.2 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.7/2.2 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 0.8/2.2 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 0.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.0/2.2 MB 1.9 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.0/2.2 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.3/2.2 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.1/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.1/2.2 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 1.9 MB/s eta 0:00:00\n",
            "Installing collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
            "     ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/10.5 MB 1.9 MB/s eta 0:00:06\n",
            "   ---------------------------------------- 0.1/10.5 MB 2.6 MB/s eta 0:00:05\n",
            "    --------------------------------------- 0.2/10.5 MB 2.0 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 0.3/10.5 MB 2.0 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 0.4/10.5 MB 1.8 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 0.5/10.5 MB 2.0 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 0.7/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 0.7/10.5 MB 2.1 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 0.9/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 1.0/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 1.1/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 1.1/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 1.2/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 1.4/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 1.4/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 1.5/10.5 MB 2.1 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 1.7/10.5 MB 2.2 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 1.7/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 1.9/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.0/10.5 MB 2.3 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.0/10.5 MB 2.3 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.1/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 2.3/10.5 MB 2.3 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 2.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 2.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 2.5/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 2.6/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 2.7/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 2.8/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 2.9/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 3.0/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 3.0/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 3.2/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 3.3/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.4/10.5 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.5/10.5 MB 2.0 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.5/10.5 MB 2.0 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.5/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.7/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 3.7/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 3.8/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.0/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.1/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.2/10.5 MB 1.9 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 4.4/10.5 MB 2.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 4.5/10.5 MB 2.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 4.6/10.5 MB 2.0 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 4.7/10.5 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 4.9/10.5 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 4.9/10.5 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 5.0/10.5 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 5.2/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 5.3/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 5.4/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 5.6/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 5.7/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 5.8/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 5.9/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 6.1/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 6.1/10.5 MB 2.1 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 6.2/10.5 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 6.3/10.5 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 6.5/10.5 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 6.6/10.5 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 6.7/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.8/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 6.9/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 7.1/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 7.2/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 7.2/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 7.4/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 7.5/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 7.6/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 7.8/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 7.9/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 8.0/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 8.1/10.5 MB 2.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 8.3/10.5 MB 2.2 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 8.3/10.5 MB 2.2 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.5/10.5 MB 2.2 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.6/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 8.7/10.5 MB 2.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 8.7/10.5 MB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 8.9/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 9.0/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 9.0/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.2/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.3/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 9.5/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 9.6/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 9.7/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 9.8/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 9.9/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.0/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.2/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.3/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.4/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.5/10.5 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.5/10.5 MB 2.2 MB/s eta 0:00:00\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "   ---------------------------------------- 0.0/362.1 kB ? eta -:--:--\n",
            "   --------- ------------------------------ 81.9/362.1 kB 4.8 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 112.6/362.1 kB 1.7 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 194.6/362.1 kB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 286.7/362.1 kB 1.8 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 307.2/362.1 kB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 362.1/362.1 kB 1.6 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
            "   ---------------------------------------- 0.0/512.1 kB ? eta -:--:--\n",
            "   ----- ---------------------------------- 71.7/512.1 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 174.1/512.1 kB 1.7 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 194.6/512.1 kB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 307.2/512.1 kB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 419.8/512.1 kB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 419.8/512.1 kB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 512.1/512.1 kB 1.6 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
            "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/273.6 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 112.6/273.6 kB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 225.3/273.6 kB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 273.6/273.6 kB 1.7 MB/s eta 0:00:00\n",
            "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
            "   ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/308.9 kB ? eta -:--:--\n",
            "   ------------------- -------------------- 153.6/308.9 kB 1.8 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 245.8/308.9 kB 1.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 256.0/308.9 kB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 308.9/308.9 kB 1.6 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.1/2.4 MB 2.2 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.2/2.4 MB 1.7 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 0.2/2.4 MB 1.5 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 0.3/2.4 MB 1.7 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 0.4/2.4 MB 1.9 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 0.6/2.4 MB 2.0 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 0.6/2.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.7/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.8/2.4 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 0.9/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 0.9/2.4 MB 1.8 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.1/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 1.2/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 1.2/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.3/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 1.4/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.5/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.6/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 1.6/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.7/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 1.8/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.9/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 2.0/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 2.0/2.4 MB 1.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.1/2.4 MB 1.8 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 2.3/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.3/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.4/2.4 MB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 1.8 MB/s eta 0:00:00\n",
            "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, accelerate, transformers\n",
            "Successfully installed accelerate-1.7.0 huggingface-hub-0.32.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tenseal\n",
        "!pip install transformers accelerate torch\n",
        "!pip install -q --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import os\n",
        "import json\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import hashlib\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMBfKLGv84uP"
      },
      "outputs": [],
      "source": [
        "def hash_ngram(ngram, buckets=1024):\n",
        "    return int(hashlib.sha256(ngram.encode()).hexdigest(), 16) % buckets\n",
        "\n",
        "def vectorize(text, n=4, buckets=1024):\n",
        "    vec = [0] * buckets\n",
        "    text = text.lower()\n",
        "    for i in range(len(text) - n + 1):\n",
        "        idx = hash_ngram(text[i:i+n], buckets)\n",
        "        vec[idx] += 1\n",
        "    return vec\n",
        "\n",
        "# Connect to OpenRouter using their API key and base URL\n",
        "client = OpenAI(\n",
        "    api_key=\"...\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# 1. Helper: LLM phrase generation from regex\n",
        "def generate_phrases_from_regex(regex: str, max_phrases=10) -> List[str]:\n",
        "    prompt = f\"\"\"You are a cybersecurity expert. Given the following regex from a phishing YARA rule:\n",
        "\n",
        "{regex}\n",
        "\n",
        "List {max_phrases} natural phrases or sentences that could appear in a phishing email and match the intent of this regex.\n",
        "Just list each phrase on a new line without explanations.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"mistralai/mistral-7b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    raw_text = response.choices[0].message.content\n",
        "\n",
        "    phrases = [\n",
        "        re.sub(r\"^\\s*[\\d]+[\\.\\)\\-]*\\s*\", \"\", line).strip(\" -‚Ä¢\\n\").lstrip(\"\\\"\")\n",
        "        for line in raw_text.split(\"\\n\") if line.strip()\n",
        "    ]\n",
        "    return phrases\n",
        "\n",
        "# 2. Helper: extract n-grams (3-5 words) from phrases\n",
        "def extract_ngrams(text: str, min_n=3, max_n=5) -> List[str]:\n",
        "    words = text.lower().split()\n",
        "    return [\n",
        "        \" \".join(words[i:i+n])\n",
        "        for n in range(min_n, max_n + 1)\n",
        "        for i in range(len(words) - n + 1)\n",
        "    ]\n",
        "\n",
        "# 3. Convert YARA regexes to vectorizer and feature space\n",
        "def vectorize_yara_phrases(rules: List[Dict[str, object]]) -> Tuple[TfidfVectorizer, List[str]]:\n",
        "    all_phrases = []\n",
        "    for rule in rules:\n",
        "        pattern = rule[\"pattern\"]\n",
        "        phrases = generate_phrases_from_regex(pattern)\n",
        "        all_phrases.extend(phrases)\n",
        "\n",
        "    # Convert to 3‚Äì5 word n-grams\n",
        "    ngrams = set()\n",
        "    for phrase in all_phrases:\n",
        "        ngrams.update(extract_ngrams(phrase, 3, 5))\n",
        "\n",
        "    sorted_vocab = sorted(ngrams)\n",
        "    vectorized_vocab = [vectorize(phrase) for phrase in sorted_vocab]\n",
        "\n",
        "    return vectorized_vocab, sorted_vocab\n",
        "\n",
        "\n",
        "\n",
        "def generate_regexs_from_rule_text(rule_text: str, max_regex=3) -> List[Dict[str, str]]:\n",
        " \n",
        "    prompt = f\"\"\"You are a cybersecurity expert. Here is a YARA rule:\n",
        "    {rule_text}\n",
        "\n",
        "    Your task is to extract up to {max_regex} regular expressions that are explicitly written or clearly implied by the rule.\n",
        "\n",
        "    For each regex, estimate the \"weight\" field (as a float) to reflect how strongly the pattern indicates phishing (the higher the weight, the more suspicious/phishy the pattern is). Assign higher weights to patterns that are more typical for phishing, and lower weights to more generic or less suspicious patterns.\n",
        "\n",
        "    Return the result as a valid Python list of dictionaries, using the following exact format:\n",
        "    [\n",
        "        {{ \"pattern\": r\"/regex1/i\", \"weight\": 1.0 }},\n",
        "        {{ \"pattern\": r\"/regex2/i\", \"weight\": 0.7 }}\n",
        "    ]\n",
        "\n",
        "    Do not include any explanations, markdown, or comments!\n",
        "\n",
        "    - Each pattern must be a Python raw string (starting with r\") and contain a YARA-style regex: between slashes (e.g., /something/i).\n",
        "    - Do NOT use extra quotes inside the pattern (e.g., no \"r\\\"...\\\"\" or escapes).\n",
        "    - Do NOT include any markdown, explanations, comments, or extra text ‚Äî just the list.\n",
        "    - If the rule contains no regex, infer up to 3 regex-style patterns that match the intent of the rule (e.g., suspicious links, phishing phrases, etc.).\n",
        "    - The pattern must be a valid Python raw string.\n",
        "    - Avoid unescaped quotes. Prefer single quotes `'` around strings.\n",
        "    - Do not include YARA flags like `nocase` as raw identifiers ‚Äî use `/.../i` inside the string instead.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"mistralai/mistral-7b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    raw_output = response.choices[0].message.content\n",
        "\n",
        "    try:\n",
        "        return eval(raw_output.strip())  # UWAGA: tylko je≈õli masz pe≈ÇnƒÖ kontrolƒô nad odpowiedziƒÖ\n",
        "    except Exception as e:\n",
        "        print(\"B≈ÇƒÖd parsowania odpowiedzi z LLM:\", e)\n",
        "        print(\"Odpowied≈∫:\\n\", raw_output)\n",
        "        return []\n",
        "\n",
        "\n",
        "def generate_regexs_from_folder(folder_path: str, max_regex=3):\n",
        "    \"\"\"\n",
        "    Zwraca listƒô krotek (nazwa_pliku, lista_regu≈Ç)\n",
        "    \"\"\"\n",
        "    all_rule_sets = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if not filename.endswith(\".yar\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            rule_text = f.read()\n",
        "\n",
        "        regex_list = generate_regexs_from_rule_text(rule_text, max_regex=max_regex)\n",
        "        if regex_list:\n",
        "            all_rule_sets.append((filename, regex_list))\n",
        "        else:\n",
        "            print(f\"Brak regex√≥w w pliku {filename}\")\n",
        "\n",
        "    return all_rule_sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6IvB0EQWkSiS"
      },
      "outputs": [],
      "source": [
        "import tenseal as ts\n",
        "import hashlib\n",
        "\n",
        "# --- Configuration ---\n",
        "BUCKETS = 1024\n",
        "NGRAM_SIZE = 4\n",
        "SCALE = 2 ** 40\n",
        "\n",
        "# --- Text Hashing Vectorizer ---\n",
        "def hash_ngram(ngram, buckets=BUCKETS):\n",
        "    return int(hashlib.sha256(ngram.encode()).hexdigest(), 16) % buckets\n",
        "\n",
        "def vectorize(text, n=NGRAM_SIZE, buckets=BUCKETS):\n",
        "    vec = [0] * buckets\n",
        "    text = text.lower()\n",
        "    for i in range(len(text) - n + 1):\n",
        "        idx = hash_ngram(text[i:i+n], buckets)\n",
        "        vec[idx] += 1\n",
        "    return vec\n",
        "\n",
        "# --- CKKS Context ---\n",
        "def create_ckks_context():\n",
        "    context = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=8192,\n",
        "        coeff_mod_bit_sizes=[60, 40, 40, 60]  # Must match scale size\n",
        "    )\n",
        "    context.global_scale = SCALE\n",
        "    context.generate_galois_keys()\n",
        "    return context\n",
        "\n",
        "# --- Encryption Helper ---\n",
        "def encrypt_vector(vector, context):\n",
        "    return ts.ckks_vector(context, vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eISOubUqb6IA"
      },
      "outputs": [],
      "source": [
        "# yara_rules = [\n",
        "#     r\"/pass(word)?\\s*reset/i\"]\n",
        "# vectorized_vocab, sorted_vocab = vectorize_yara_phrases(yara_rules)\n",
        "# print(sorted_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "hbY3bzti6uRG",
        "outputId": "c0a1733a-3ee4-4769-84b6-ac0831027cb3"
      },
      "outputs": [],
      "source": [
        "# --- YARA rules with weights ---\n",
        "rules = [\n",
        "    {\"pattern\": r\"/pass(word)?\\s*reset/i\", \"weight\": 1.5},\n",
        "    {\"pattern\": r\"/account\\s+(suspended|locked|restricted)/i\", \"weight\": 1.0},\n",
        "    {\"pattern\": r\"/verify\\s+your\\s+identity/i\", \"weight\": 1.2},\n",
        "    {\"pattern\": r\"/click\\s+the\\s+link/i\", \"weight\": 0.8},\n",
        "    {\"pattern\": r\"/secure\\s+login/i\", \"weight\": 1.3},\n",
        "]\n",
        "vectorized_vocab, sorted_vocab = vectorize_yara_phrases(rules)\n",
        "# print(sorted_vocab)\n",
        "\n",
        "\n",
        "\n",
        "def rule_matches(pattern: str, text: str) -> bool:\n",
        "    return re.search(pattern.strip(\"/i\"), text, re.IGNORECASE) is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Zestaw regu≈Ç z pliku: complex_html_rule.yar ---\n",
            "Regexy w zestawie:\n",
            "  <!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01\n",
            "  <a[^>]+>\n",
            "  (Hello,|Best regards,)\n",
            "\n",
            "--- Zestaw regu≈Ç z pliku: domains_rule.yar ---\n",
            "Regexy w zestawie:\n",
            "  chainsmokers\\-feeling\\.org\n",
            "  [a-z0-9]+(?:\\.|_){2,}[a-z]{2,}\\b\n",
            "  (?:amazonses|sendgrid|sparkpostmail)\\.com\n",
            "\n",
            "--- Zestaw regu≈Ç z pliku: encoded_reply_rule.yar ---\n",
            "Regexy w zestawie:\n",
            "  /Reply-To:\\s*=\\?UTF-8\\?B\\?.{20,}\\?=/i\n",
            "  /Reply-To:\\s*=\\?.*\\?.{20,}\\?=/i\n",
            "  /Reply-To:\\s*=.*\\?B\\?.{20,}\\?=/i\n",
            "\n",
            "--- Zestaw regu≈Ç z pliku: phrases_rule.yar ---\n",
            "Regexy w zestawie:\n",
            "  \"Renew your subscription\"\n",
            "  \"Update your payment details\"\n",
            "  \"Password Expiration Notification\"\n",
            "  \"Click below\"\n",
            "\n",
            "--- Zestaw regu≈Ç z pliku: sekurak_yara_example.yar ---\n",
            "Regexy w zestawie:\n",
            "  \"sekret\"\n",
            "  \"password\"\n",
            "  \"sekret.*password\"\n",
            "\n",
            "--- Zestaw regu≈Ç z pliku: suspicious_links_rule.yar ---\n",
            "Regexy w zestawie:\n",
            "  bit\\.ly/\n",
            "  tinyurl\\.com/\n",
            "  https?://[^ ]*@(.*)\n",
            "\n",
            "--- Zestaw regu≈Ç z pliku: suspicious_tld_rule.yar ---\n",
            "Regexy w zestawie:\n",
            "  $tld_ml|$tld_ga|$tld_ru|$tld_su|$tld_skin|$tld_cn|$tld_top|$tld_sbs|$tld_bond|$tld_xyz\n",
            "  \\.com(?!\\.[a-zA-Z]{2,3})\n",
            "  [a-zA-Z0-9]+[-._]?[a-zA-Z0-9]+[-._]?[a-zA-Z0-9]+\\.($tld_ml|$tld_ga|$tld_ru|$tld_su|$tld_skin|$tld_cn|$tld_top|$tld_sbs|$tld_bond|$tld_xyz)\n"
          ]
        }
      ],
      "source": [
        "# -- YARA rules with weights for every file in the \"rules\" folder ---\n",
        "rules_sets = generate_regexs_from_folder(\"rules/\", max_regex=3)\n",
        "\n",
        "all_vectorized_vocabs = []  # lista: [vectorized_vocab dla ka≈ºdego pliku]\n",
        "all_sorted_vocabs = []      # lista: [sorted_vocab dla ka≈ºdego pliku]\n",
        "all_filenames = []          # lista nazw plik√≥w \n",
        "all_rules = []              # lista regu≈Ç \n",
        "\n",
        "for filename, rules in rules_sets:\n",
        "    print(f\"\\n--- Zestaw regu≈Ç z pliku: {filename} ---\")\n",
        "    print(\"Regexy w zestawie:\")\n",
        "    for rule in rules:\n",
        "        print(f\"  {rule['pattern']}\")\n",
        "    vectorized_vocab, sorted_vocab = vectorize_yara_phrases(rules)\n",
        "    all_vectorized_vocabs.append(vectorized_vocab)\n",
        "    all_sorted_vocabs.append(sorted_vocab)\n",
        "    all_filenames.append(filename)\n",
        "    all_rules.extend(rules)\n",
        "    # print(\"Lista fraz:\", sorted_vocab)\n",
        "\n",
        "\n",
        "# -- YARA rules with weights for all files in the \"rules\" folder ---\n",
        "global_vectorized_vocab, global_sorted_vocab = vectorize_yara_phrases(all_rules)\n",
        "# print(\"\\n=== GLOBALNA LISTA S≈ÅOWNIK√ìW FRAZ (ze wszystkich plik√≥w) ===\")\n",
        "# print(global_sorted_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plik: domains_rule.yar \n",
            "Lista s≈Çownik√≥w regex√≥w z wagami: [{'pattern': 'chainsmokers\\\\-feeling\\\\.org', 'weight': 1.0}, {'pattern': '[a-z0-9]+(?:\\\\.|_){2,}[a-z]{2,}\\\\b', 'weight': 0.8}, {'pattern': '(?:amazonses|sendgrid|sparkpostmail)\\\\.com', 'weight': 0.7}]\n"
          ]
        }
      ],
      "source": [
        "#--- Example usage ---\n",
        "print(F\"Plik: {rules_sets[1][0]} \\nLista s≈Çownik√≥w regex√≥w z wagami: {rules_sets[1][1]}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmkkYTUlYNwA",
        "outputId": "d3d80a4e-c603-4611-95f8-3d5a6a94d4c3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity Score (Dot Product, decrypted): 36.00000536888277\n",
            "\n",
            "--- Results ---\n",
            "Similarity score: 36.00\n",
            "YARA rules matched: 0 (total weight: 0.00)\n",
            "Combined score: 36.00\n",
            "Threshold: 10.00 (similarity), 12.00 (combined)\n",
            "\n",
            "PHISHING!\n",
            "Matched YARA rules:\n"
          ]
        }
      ],
      "source": [
        "# --- Main Logic ---\n",
        "context = create_ckks_context()\n",
        "\n",
        "email_text = \"\"\"Subject: Immediate Action Required ‚Äì Password Reset\n",
        "\n",
        "Dear user,\n",
        "\n",
        "We have detected suspicious activity on your account and, as a security precaution, your access has been temporarily limited.\n",
        "\n",
        "To restore access, please follow the link below to initiate a password reset:\n",
        "\n",
        "üëâ https://secure-login-authenticator.com/reset\n",
        "\n",
        "If you do not reset your password within 24 hours, your account will be permanently locked.\n",
        "\n",
        "Thank you for your cooperation,\n",
        "Security Team\n",
        "\"\"\"\n",
        "yara_rule_text = \"password reset\"\n",
        "\n",
        "# Vectorize\n",
        "email_vector = vectorize(email_text)\n",
        "yara_vector = vectorize(yara_rule_text)\n",
        "\n",
        "# Pad to same length if needed (shouldn't be necessary here)\n",
        "if len(email_vector) != len(yara_vector):\n",
        "    max_len = max(len(email_vector), len(yara_vector))\n",
        "    email_vector += [0] * (max_len - len(email_vector))\n",
        "    yara_vector += [0] * (max_len - len(yara_vector))\n",
        "\n",
        "# Encrypt both vectors\n",
        "enc_email = encrypt_vector(email_vector, context)\n",
        "enc_yara = encrypt_vector(yara_vector, context)\n",
        "\n",
        "# Compute encrypted dot product\n",
        "# This works because both vectors have same scale & context\n",
        "enc_score = enc_email.dot(enc_yara)\n",
        "\n",
        "# --- Threshold Configuration ---\n",
        "THRESHOLD = 10.0\n",
        "\n",
        "# Decrypt the result\n",
        "score = enc_score.decrypt()[0]\n",
        "print(\"Similarity Score (Dot Product, decrypted):\", score)\n",
        "\n",
        "# Check against threshold\n",
        "is_phishing = score >= THRESHOLD\n",
        "\n",
        "# Additional verification with YARA rules\n",
        "yara_matches = [\n",
        "    rule for rule in rules\n",
        "    if rule_matches(rule[\"pattern\"], email_text)\n",
        "]\n",
        "yara_score = sum(rule[\"weight\"] for rule in yara_matches)\n",
        "\n",
        "# Combined approach: similarity + YARA rules\n",
        "final_score = score + yara_score\n",
        "final_threshold = THRESHOLD + 2.0  # Higher threshold for combined approach\n",
        "\n",
        "is_phishing_final = final_score >= final_threshold\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "print(f\"Similarity score: {score:.2f}\")\n",
        "print(f\"YARA rules matched: {len(yara_matches)} (total weight: {yara_score:.2f})\")\n",
        "print(f\"Combined score: {final_score:.2f}\")\n",
        "print(f\"Threshold: {THRESHOLD:.2f} (similarity), {final_threshold:.2f} (combined)\")\n",
        "if is_phishing_final:\n",
        "    print(\"\\nPHISHING!\")\n",
        "    print(\"Matched YARA rules:\")\n",
        "    for rule in yara_matches:\n",
        "        print(f\"- {rule['pattern']} (weight: {rule['weight']})\")\n",
        "else:\n",
        "    print(\"\\nEmail appears legitimate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LXuKyMFP6uRH",
        "outputId": "33deafd8-6a71-4e79-c031-9f8509f45a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results ---\n",
            "Max similarity score fraz: 96.00\n",
            "YARA rules matched: 0 (total weight: 0.00)\n",
            "Combined score: 96.00\n",
            "Threshold: 10.00 (similarity), 12.00 (combined)\n",
            "\n",
            "PHISHING!\n",
            "Matched YARA rules:\n"
          ]
        }
      ],
      "source": [
        "# Padding\n",
        "max_len = max(len(email_vector), len(vectorized_vocab[0]))\n",
        "email_vector += [0] * (max_len - len(email_vector))\n",
        "\n",
        "enc_email = encrypt_vector(email_vector, context)\n",
        "\n",
        "# Calculate similarity with each phrase vector from the vocab:\n",
        "scores = []\n",
        "for phrase_vec in vectorized_vocab:\n",
        "    vec = phrase_vec\n",
        "    vec += [0] * (max_len - len(vec))  # padding\n",
        "    enc_phrase = encrypt_vector(vec, context)\n",
        "    enc_score = enc_email.dot(enc_phrase)\n",
        "    score = enc_score.decrypt()[0]\n",
        "    scores.append(score)\n",
        "\n",
        "max_score = max(scores)\n",
        "\n",
        "# Check which YARA rules match the email text\n",
        "yara_matches = [rule for rule in rules if rule_matches(rule[\"pattern\"], email_text)]\n",
        "yara_score = sum(rule[\"weight\"] for rule in yara_matches)\n",
        "\n",
        "# Calculate the combined score\n",
        "final_score = max_score + yara_score\n",
        "\n",
        "THRESHOLD = 10.0\n",
        "final_threshold = THRESHOLD + 2.0\n",
        "\n",
        "is_phishing_final = final_score >= final_threshold\n",
        "\n",
        "print(\"\\n--- Results ---\")\n",
        "print(f\"Max similarity score fraz: {max_score:.2f}\")\n",
        "print(f\"YARA rules matched: {len(yara_matches)} (total weight: {yara_score:.2f})\")\n",
        "print(f\"Combined score: {final_score:.2f}\")\n",
        "print(f\"Threshold: {THRESHOLD:.2f} (similarity), {final_threshold:.2f} (combined)\")\n",
        "if is_phishing_final:\n",
        "    print(\"\\nPHISHING!\")\n",
        "    print(\"Matched YARA rules:\")\n",
        "    for rule in yara_matches:\n",
        "        print(f\"- {rule['pattern']} (weight: {rule['weight']})\")\n",
        "else:\n",
        "    print(\"\\nEmail appears legitimate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r8uh8XO_8CfC"
      },
      "outputs": [],
      "source": [
        "def vectorize_all_emails(input_dir: str, output_path: str) -> None:\n",
        "    input_dir = Path(input_dir)\n",
        "    vectors: list[np.ndarray] = []\n",
        "    for dirpath, dirnames, filenames in os.walk(input_dir):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            with open(file_path, encoding=\"utf-8\", errors=\"replace\") as file:\n",
        "                text = file.read()\n",
        "                vec = vectorize(text)\n",
        "                vectors.append(np.array(vec))\n",
        "\n",
        "    np.savez_compressed(output_path, np.array(vectors))\n",
        "    \n",
        "    return np.array(vectors)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iter_emails(path: str):\n",
        "    array = np.load(path)['arr_0']\n",
        "    for row in array:\n",
        "        yield row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7toFsatX8D_O"
      },
      "outputs": [],
      "source": [
        "phishing = vectorize_all_emails(\n",
        "    input_dir=\"samples/phishing_mails\",\n",
        "    output_path=\"./samples/phishing_mails_vectorized\"\n",
        ")\n",
        "\n",
        "regular = vectorize_all_emails(\n",
        "    input_dir=\"./samples/regular_mails\",\n",
        "    output_path=\"./samples/regular_mails_vectorized\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "mail_vectors = iter_emails('./samples/phishing_mails_vectorized.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate_threshold(regular_path, vectorized_vocab, vocab_boost=2.0, percentile=75):\n",
        "    context = create_ckks_context()\n",
        "    max_len = max(max(len(vec) for vec in vectorized_vocab), BUCKETS)\n",
        "\n",
        "    padded_vocab = [vec + [0] * (max_len - len(vec)) for vec in vectorized_vocab]\n",
        "    encrypted_vocab = [encrypt_vector(vec, context) for vec in padded_vocab]\n",
        "\n",
        "    def evaluate(vec):\n",
        "        vec = list(vec) + [0] * (max_len - len(vec))\n",
        "        enc_email = encrypt_vector(vec, context)\n",
        "        similarity_scores = [enc_email.dot(enc_phrase).decrypt()[0] for enc_phrase in encrypted_vocab]\n",
        "        return max(similarity_scores)\n",
        "\n",
        "    similarities = []\n",
        "    for vec in iter_emails(regular_path):\n",
        "        similarities.append(evaluate(vec))\n",
        "\n",
        "    import numpy as np\n",
        "    base_threshold = np.percentile(similarities, percentile)\n",
        "\n",
        "    return base_threshold + vocab_boost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scan_vectorized_emails(\n",
        "    phishing_path: str,\n",
        "    regular_path: str,\n",
        "    output_path: str,\n",
        "    rules: List[Dict[str, object]],\n",
        "    vectorized_vocab: List[List[int]],\n",
        "    threshold: float = 10.0,\n",
        "    vocab_boost: float = 2.0,\n",
        "    sample_fraction: float = 1.0,\n",
        "    max_files: int = None\n",
        "):\n",
        "    import random\n",
        "\n",
        "    context = create_ckks_context()\n",
        "    max_len = max(max(len(vec) for vec in vectorized_vocab), BUCKETS)\n",
        "\n",
        "    padded_vocab = [vec + [0] * (max_len - len(vec)) for vec in vectorized_vocab]\n",
        "    encrypted_vocab = [encrypt_vector(vec, context) for vec in padded_vocab]\n",
        "\n",
        "    def evaluate_email(vec):\n",
        "        vec = list(vec) + [0] * (max_len - len(vec))\n",
        "        enc_email = encrypt_vector(vec, context)\n",
        "        similarity_scores = [enc_email.dot(enc_phrase).decrypt()[0] for enc_phrase in encrypted_vocab]\n",
        "        return max(similarity_scores)\n",
        "\n",
        "    if threshold == \"auto\":\n",
        "        threshold = calibrate_threshold(\n",
        "            regular_path, vectorized_vocab,\n",
        "            vocab_boost=vocab_boost\n",
        "        )\n",
        "        print(f\"Dynamicznie ustawiony pr√≥g: {threshold:.2f}\")\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for label, path in [(\"phishing_mails\", phishing_path), (\"regular_mails\", regular_path)]:\n",
        "            all_vecs = list(iter_emails(path))\n",
        "            random.shuffle(all_vecs)\n",
        "            sample_size = min(int(len(all_vecs) * sample_fraction), max_files or len(all_vecs))\n",
        "\n",
        "            for idx, vec in enumerate(all_vecs[:sample_size]):\n",
        "                similarity = evaluate_email(vec)\n",
        "                yara_score = 0.0\n",
        "                text = \"\"  # optionally left empty\n",
        "                for rule in rules:\n",
        "                    if rule_matches(rule[\"pattern\"], text):\n",
        "                        yara_score += rule[\"weight\"]\n",
        "\n",
        "                final_score = similarity + yara_score\n",
        "                final_threshold = threshold\n",
        "                result = \"MATCH\" if final_score >= final_threshold else \"NO MATCH\"\n",
        "\n",
        "                f.write(f\"{result}: {label}/{idx} -> similarity={similarity:.2f}, yara_score={yara_score:.2f}, total={final_score:.2f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_vectorized_results(result_file_path: str):\n",
        "    phishing_matches = phishing_total = 0\n",
        "    regular_matches = regular_total = 0\n",
        "    other_matches = other_total = 0\n",
        "\n",
        "    with open(result_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            is_match = line.startswith(\"MATCH\")\n",
        "            if \"phishing_mails\" in line:\n",
        "                phishing_total += 1\n",
        "                if is_match:\n",
        "                    phishing_matches += 1\n",
        "            elif \"regular_mails\" in line:\n",
        "                regular_total += 1\n",
        "                if is_match:\n",
        "                    regular_matches += 1\n",
        "            else:\n",
        "                other_total += 1\n",
        "                if is_match:\n",
        "                    other_matches += 1\n",
        "\n",
        "    total_matches = phishing_matches + regular_matches + other_matches\n",
        "    total_scanned = phishing_total + regular_total + other_total\n",
        "\n",
        "    print(\"Analiza wynik√≥w (vectorized approach):\")\n",
        "    print(f\"Phishing mails: {phishing_matches} / {phishing_total} dopasowa≈Ñ\")\n",
        "    print(f\"Regular mails:  {regular_matches} / {regular_total} dopasowa≈Ñ\")\n",
        "    print(f\"Inne pliki:      {other_matches} / {other_total} dopasowa≈Ñ\")\n",
        "    print(f\"SUMA:            {total_matches} / {total_scanned} plik√≥w dopasowanych\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---All YARA files processing in loop--\n",
        "for (filename, rules), vectorized_vocab in zip(rules_sets, all_vectorized_vocabs):\n",
        "    print(f\"\\n--- Przetwarzanie regu≈Ç z pliku: {filename} ---\")\n",
        "    output_path = f\"results/{filename}_results.txt\"\n",
        "    \n",
        "    scan_vectorized_emails(\n",
        "        phishing_path=\"./samples/phishing_mails_vectorized.npz\",\n",
        "        regular_path=\"./samples/regular_mails_vectorized.npz\",\n",
        "        output_path=output_path,\n",
        "        rules=rules,\n",
        "        vectorized_vocab=vectorized_vocab,\n",
        "        threshold=\"auto\",\n",
        "        vocab_boost=2.0\n",
        "    )\n",
        "    \n",
        "    analyze_vectorized_results(output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Global YARA rules processing ---\n",
        "scan_vectorized_emails(\n",
        "    phishing_path=\"./samples/phishing_mails_vectorized.npz\",\n",
        "    regular_path=\"./samples/regular_mails_vectorized.npz\",\n",
        "    output_path=\"results/global_results.txt\",\n",
        "    rules=rules,\n",
        "    vectorized_vocab=global_vectorized_vocab,\n",
        "    threshold=\"auto\",\n",
        "    vocab_boost=2.0\n",
        ")\n",
        "\n",
        "analyze_vectorized_results(\"results/global_results.txt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
